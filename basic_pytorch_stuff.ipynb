{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Basic tutorials for basic PyTorch stuff: Cross-Entropy loss, Softmax, UpSample, cropping, concatenation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8k0_LrMGgX_"
      },
      "source": [
        "Rough code to understand cross-entropy loss in torch. Btw, cross-entropy in torch does log-softmax by itself, so you can send the \"raw\" output directly. The log used is natural log. Log of base 2 could also be used, but shouldn't really matter: https://stats.stackexchange.com/questions/295174/difference-in-log-base-for-cross-entropy-calcuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDY3dCMUfuVV",
        "outputId": "4c948a55-d59b-470b-8314-2945aed9b1c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 2, 2, 2]) torch.Size([1, 2, 2, 2])\n",
            "tensor([[[0.6163, 0.6027],\n",
            "         [0.5130, 0.7236]]])\n",
            "tensor([[[0.6163, 0.6027],\n",
            "         [0.5130, 0.7236]]])\n",
            "tensor(2.4556)\n",
            "tensor(2.4556)\n"
          ]
        }
      ],
      "source": [
        "#predicted output\n",
        "x = torch.tensor([[[0.41, 0.56], [0.69, 0.84]], [[0.57, 0.37], [0.29, 0.90]]])\n",
        "x = x.unsqueeze(0) #The generic way to store image tensors is with a batch size too.\n",
        "\n",
        "#You can either give the actual probabilities.\n",
        "y = torch.tensor([[[0., 1.], [1., 1.]],[[1., 0.], [0., 0.]]]) #target output\n",
        "y = y.unsqueeze(0) #Same reason as above.\n",
        "\n",
        "#Or you can specify which class the pixels belong to. The first class has\n",
        "#index 0, the second class has index 1, and so on. This is better computationally.\n",
        "#If you are 100% sure in your target output of the classes of the pixel i.e. you use\n",
        "#0 or 1, then it is better to use this.\n",
        "y_class_index = torch.tensor([[1, 0], [0, 0]]) #target output\n",
        "y_class_index = y_class_index.unsqueeze(0) #interpreted as [batch, height, width]\n",
        "\n",
        "#reduction 'none' gives you the cross entropy value of each pixel.\n",
        "entropy_1 = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "#reduction 'sum' gives you the cross entropy value of the whole image.\n",
        "entropy_2 = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "#The outputs are the same, but the \"class_index\" approach is computationally\n",
        "#better. Look at the \"NOTE\" portion here:\n",
        "#https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "#It makes sense that it is computationally better too. If the target corresponding\n",
        "#to an output is 0, then that output doesn't contribute to the cross-entropy\n",
        "#calculation (look at the formula for cross-entropy.) Therefore, you can\n",
        "#ignore those. With the class index approach, you are essentially telling\n",
        "#pytorch that your intention is to ignore the ignorable.\n",
        "print(entropy_1(x, y))\n",
        "print(entropy_1(x, y_class_index))\n",
        "\n",
        "print(entropy_2(x, y))\n",
        "print(entropy_2(x, y_class_index))\n",
        "\n",
        "#Interesting that stuff in pytorch is so nicely structured to be straightforward\n",
        "#for images. But maybe that's a bias because I'm only doing stuff for images.\n",
        "#Maybe stuff is straightforward for other stuff too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh3ufr_HNICJ"
      },
      "source": [
        "Rough code to understand how softmax works in torch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBH9FtbhNHl5",
        "outputId": "371d680f-1617-4eab-aac1-6ee47dd522f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[0.4452, 0.3417],\n",
            "          [0.5520, 0.3510]],\n",
            "\n",
            "         [[0.4771, 0.5547],\n",
            "          [0.5214, 0.5568]]],\n",
            "\n",
            "\n",
            "        [[[0.5548, 0.6583],\n",
            "          [0.4480, 0.6490]],\n",
            "\n",
            "         [[0.5229, 0.4453],\n",
            "          [0.4786, 0.4432]]]])\n",
            "tensor([[[[0.4998, 0.3139],\n",
            "          [0.5768, 0.4049]],\n",
            "\n",
            "         [[0.5002, 0.6861],\n",
            "          [0.4232, 0.5951]]],\n",
            "\n",
            "\n",
            "        [[[0.5319, 0.5233],\n",
            "          [0.5465, 0.6124]],\n",
            "\n",
            "         [[0.4681, 0.4767],\n",
            "          [0.4535, 0.3876]]]])\n",
            "tensor([[[[0.4083, 0.4500],\n",
            "          [0.5917, 0.5500]],\n",
            "\n",
            "         [[0.4848, 0.5488],\n",
            "          [0.5152, 0.4512]]],\n",
            "\n",
            "\n",
            "        [[[0.5144, 0.4601],\n",
            "          [0.4856, 0.5399]],\n",
            "\n",
            "         [[0.5291, 0.5509],\n",
            "          [0.4709, 0.4491]]]])\n",
            "tensor([[[[0.4966, 0.5034],\n",
            "          [0.5391, 0.4609]],\n",
            "\n",
            "         [[0.3112, 0.6888],\n",
            "          [0.3687, 0.6313]]],\n",
            "\n",
            "\n",
            "        [[[0.3896, 0.6104],\n",
            "          [0.3393, 0.6607]],\n",
            "\n",
            "         [[0.3814, 0.6186],\n",
            "          [0.4023, 0.5977]]]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(2, 2, 2, 2)\n",
        "\n",
        "#Softmax occurs between batches. For instance, the\n",
        "#softmax for the value at the 1st pixel position in the 1st channel\n",
        "#of the 1st batch would be between the values at the 1st pixel position\n",
        "#in the 1st channel of all the 'N' batches.\n",
        "soft_1 = nn.Softmax(dim=0)\n",
        "\n",
        "#Softmax occurs between channels. For instance, the\n",
        "#softmax for the value at the 1st pixel position in the 1st channel\n",
        "#of some batch would be between the values at the 1st pixel position\n",
        "#in all the channels of this batch.\n",
        "soft_2 = nn.Softmax(dim=1)\n",
        "\n",
        "#Softmax occurs amongst the values along the height of each channel.\n",
        "soft_3 = nn.Softmax(dim=2)\n",
        "\n",
        "#Softmax occurs amongst the values along the width of each channel.\n",
        "soft_4 = nn.Softmax(dim=3)\n",
        "\n",
        "print(soft_1(x))\n",
        "print(soft_2(x))\n",
        "print(soft_3(x))\n",
        "print(soft_4(x))\n",
        "\n",
        "#Btw, the values that you use to softmax sum up to 1 after softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZFOzovSCcCs"
      },
      "source": [
        "Rough code to understand how the upsample class works in torch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "schwyXwi6bQa",
        "outputId": "7b73c4a0-7d2d-4b27-d692-7c2b2e070cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 447, 640])\n",
            "torch.Size([1, 3, 894, 1280])\n"
          ]
        }
      ],
      "source": [
        "img = Image.open(requests.get(\"https://c8.alamy.com/zooms/9/7df650603bfe4193bab024ee29da5461/2btr9xg.jpg\", stream=True).raw)\n",
        "tensor_trans = transforms.ToTensor() #converting the image to a tensor\n",
        "img = tensor_trans(img) #converting the image to a tensor continued\n",
        "img = img.unsqueeze(0) #adding a batch dimension to the image.\n",
        "#Doing this because of the interpertation scheme of the shape values of tensors\n",
        "#by the Upsample class. If our image's shape was (x, y, z), Upsample would interpret\n",
        "#'x' as batch size, 'y' as number of channels and 'z' as the width. But obviously\n",
        "#'x' is number of channels, 'y' is the height and 'z' is the width. But if the shape\n",
        "#is (w, x, y, z), Upsample interprets 'w' as batch size, and 'x', 'y' and 'z' as wanted\n",
        "#above. Adding batch size is fine; it doesn't remove generality. If you only\n",
        "#want to upsample a single image, the batch size is just going to be 1. Btw, in\n",
        "#the (x, y, z) case, Upsample only scales the width -- both \"theoretically\" and\n",
        "#pratically, having a 3d tensor doesn't work for our purpose. For more clarification\n",
        "#and information: https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html\n",
        "\n",
        "print(img.shape)\n",
        "#by default, nearest is used. scale factor defines by what factor we want to increase\n",
        "#the height/width. new_height_width = old_height_width * scale_factor.\n",
        "upscale = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "img = upscale(img) #upsampling\n",
        "print(img.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7erVYxI9fcg"
      },
      "source": [
        "Rough code to understand cropping in torch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZYugYeT9mly",
        "outputId": "3c516ae3-f77f-4641-93ce-927bde59573c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 512, 56, 56])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(1, 512, 64, 64)\n",
        "#Look at the dotted lines in the paper Fig.1. I think that implies Center Crop.\n",
        "crop = transforms.CenterCrop(56)\n",
        "x = crop(x)\n",
        "print(x.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjZfm_7rHEOe"
      },
      "source": [
        "Rough code to understand how to do concat as required by the u-net paper in torch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hSV5VVnZVgt",
        "outputId": "a4ffb1cb-cbc4-47cf-a5be-36b13717a23a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.]]]])\n",
            "torch.Size([2, 2, 4, 4])\n",
            "tensor([[[[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.]]]])\n",
            "torch.Size([1, 4, 4, 4])\n",
            "tensor([[[[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.]]]])\n",
            "torch.Size([1, 2, 8, 4])\n",
            "tensor([[[[1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 0., 0., 0., 0.]],\n",
            "\n",
            "         [[1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 0., 0., 0., 0.]]]])\n",
            "torch.Size([1, 2, 4, 8])\n"
          ]
        }
      ],
      "source": [
        "x = torch.ones(1, 2, 4, 4)\n",
        "y = torch.zeros(1, 2, 4, 4)\n",
        "\n",
        "#the dimension in torch.cat refer to the shape values above. Remember, shape values are\n",
        "#(batch_size, num_of_channels, height, width).\n",
        "\n",
        "#This does the concat on 'batch level' i.e. the new tensor will contain two image tensors x and y.\n",
        "print(torch.cat((x, y), 0))\n",
        "print(torch.cat((x, y), 0).shape)\n",
        "\n",
        "#This does the concat on 'channel level' i.e. the channels of y will be added to x\n",
        "#to create a new image tensor with more channels, specifically, new_number_of_\n",
        "#channels = no_of_channels_x + no_of_channels_y\n",
        "print(torch.cat((x, y), 1))\n",
        "print(torch.cat((x, y), 1).shape)\n",
        "\n",
        "#Concat on 'height level' i.e. rows of the 1st channel of y will be added to the rows\n",
        "#of the 1st channel of x vertically. This is done similarly for each channel.\n",
        "print(torch.cat((x,y), 2))\n",
        "print(torch.cat((x, y), 2).shape)\n",
        "\n",
        "#Concat on 'width level' i.e. rows of the 1st channel of y will be added to the rows\n",
        "#of the 1st channel of x horizontally. This is done similarly for each channel.\n",
        "print(torch.cat((x, y), 3))\n",
        "print(torch.cat((x, y), 3).shape)\n",
        "\n",
        "#Look at the outputs to understand more. It also really helps if you understand\n",
        "#how pytorch tensors are structured i.e. what each pair of [] represents -- specifically\n",
        "#in the context of images I guess in our case.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
